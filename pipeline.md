# Baby MLOps Pipeline Overview

This document describes the components of the Kubeflow Pipeline (KFP) used for the Baby Weight Prediction project. The pipeline includes two parallel branches: a BigQuery ML (BQML) branch and an AutoML branch, both performing data extraction, preprocessing, model training, evaluation, and metrics collection.

## Pipeline Flow

The pipeline consists of the following components:

1. **Extract Source Data** (`extract_source_data`)
2. **Preprocess and Split Data** (`preprocess_data_and_split`)

**BQML Branch:**
3. **Train BQML Model** (`BigqueryCreateModelJobOp`)
4. **Evaluate BQML Model** (`BigqueryEvaluateModelJobOp`)
5. **Collect BQML Metrics** (`collect_eval_metrics_bqml`)

**AutoML Branch:**
6. **Create Vertex AI Dataset** (`TabularDatasetCreateOp`)
7. **Train AutoML Model** (`AutoMLTabularTrainingJobRunOp`)
8. **Collect AutoML Metrics** (`collect_eval_metrics_automl`)

**Model Selection:**
9. **Select Best Model** (`select_best_model`)

**Deployment:**
10. **Create/Check Endpoint** (dual approach with `EndpointCreateOp` and `get_or_create_endpoint`)
11. **Register Model** (`register_best_model_in_registry`) - registers the best model with proper metadata
12. **Deploy Model** (`ModelDeployOp`) - conditionally executed based on model selection
13. **Update Traffic Split** (`update_traffic_split`) - manages traffic for existing endpoints

## Component Details

### 1. Extract Source Data

*   **Component Function:** `src.pipeline_2025.data_prep_comp.extract_source_data`
*   **Description:** Extracts data from the source BigQuery table (`bigquery-public-data.samples.natality`), filters it based on a specified year and other conditions (e.g., valid `weight_pounds`, `mother_age`), and stores the result in a new BigQuery table.
*   **Inputs (from pipeline parameters):**
    *   `project_id` (str): GCP Project ID.
    *   `source_bq_table_id` (str): Full ID of the source natality table.
    *   `extracted_bq_table_id` (str): Full ID for the output table where extracted data will be stored.
    *   `filter_year` (int): Year to filter data (e.g., extract records where `year > filter_year`).
    *   `region` (str): GCP region (explicitly uses "US" for the BQ job to access public data).
*   **Outputs:**
    *   `extracted_table_uri` (str): URI of the created BigQuery table (e.g., `bq://project.dataset.table`).
    *   `extracted_table_id` (str): Full ID of the created BigQuery table.
*   **Key Operations:**
    *   Runs a `CREATE OR REPLACE TABLE AS SELECT ...` BigQuery query.
    *   Selects relevant features and applies initial filters.
    *   Ensures the target dataset exists in the "US" location.

### 2. Preprocess and Split Data

*   **Component Function:** `src.pipeline_2025.data_prep_comp.preprocess_data_and_split`
*   **Description:** Takes the extracted data, performs further preprocessing and feature engineering, limits the dataset size, and splits the data into `TRAIN`, `VALIDATE`, and `TEST` sets using a hash-based method for reproducibility.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `input_bq_table_id` (str): Full ID of the table from the "Extract Source Data" step (passed from `extract_task.outputs["extracted_table_id"]`).
    *   `preprocessed_bq_table_id` (str): Full ID for the output table where preprocessed data will be stored.
    *   `data_limit` (int): Maximum number of rows to process.
    *   `region` (str): GCP region for the BigQuery job.
*   **Outputs:**
    *   `preprocessed_table_uri` (str): URI of the created preprocessed BigQuery table.
    *   `preprocessed_table_id` (str): Full ID of the created preprocessed BigQuery table.
*   **Key Operations:**
    *   Runs a `CREATE OR REPLACE TABLE AS SELECT ...` BigQuery query.
    *   Casts boolean columns to `STRING`.
    *   Transforms `plurality` into categories (e.g., "Single(1)", "Twins(2)").
    *   Handles `NULL` values for `cigarette_use` and `alcohol_use` by casting to `STRING` and replacing `NULL` with "Unknown".
    *   Creates a `hash_values` column based on several features for reproducible data splitting.
    *   Creates a `data_split` column (`TRAIN`, `VALIDATE`, `TEST`) based on `MOD(hash_values, 10)`.

## BQML Branch Components

### 3. Train BQML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.bigquery.BigqueryCreateModelJobOp` (Pre-built GCPC component)
*   **Description:** Trains a BigQuery ML model using the preprocessed data. The specific model type and training options are defined in a SQL query string.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): BigQuery location for the job (e.g., "US").
    *   `query` (str): A SQL `CREATE MODEL` statement. This query is dynamically generated by `src.pipeline_2025.create_bqml_comp.create_query_build_bqml_model` using:
        *   `project_id`
        *   `bq_dataset` (staging dataset name from config)
        *   `bq_model_name` (from pipeline parameters)
        *   `formatted_bq_version_aliases` (from pipeline parameters, e.g., "['v1','latest']")
        *   `var_target` (target variable name from pipeline parameters, e.g., "weight_pounds")
        *   `bq_train_table_id` (the preprocessed table from `preprocess_task.outputs["preprocessed_table_id"]`)
*   **Outputs (Standard for `BigqueryCreateModelJobOp`):**
    *   `model` (Artifact): An artifact representing the trained BQML model, registered in Vertex AI Model Registry. Its URI points to the Vertex AI Model resource.
    *   Other outputs like `job_url`.
*   **Key Operations:**
    *   Executes the `CREATE MODEL` query in BigQuery.
    *   The model specified is a `DNN_LINEAR_COMBINED_REGRESSOR`.
    *   Uses `model_registry = 'vertex_ai'` to register the model with Vertex AI.
    *   Assigns `vertex_ai_model_version_aliases`.
    *   Uses `data_split_method = 'CUSTOM'` with `data_split_col = 'custom_splits'` (remapped from the `data_split` column where 'VALIDATE' becomes 'EVAL' for BQML).
    *   Includes hyperparameter tuning options (`HPARAM_CANDIDATES`, `MAX_ITERATIONS`, `NUM_TRIALS`, etc.).

### 4. Evaluate BQML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.bigquery.BigqueryEvaluateModelJobOp` (Pre-built GCPC component)
*   **Description:** Evaluates the trained BQML model.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): BigQuery location (same as training).
    *   `model` (Artifact): The trained model artifact from the "Train BQML Model" step (`bqml_train_task.outputs["model"]`).
*   **Outputs (Standard for `BigqueryEvaluateModelJobOp`):**
    *   `evaluation_metrics` (Artifact): An artifact containing the evaluation metrics from `ML.EVALUATE`.
    *   Other outputs like `job_url`.
*   **Key Operations:**
    *   Runs an `ML.EVALUATE` query on the trained BQML model using the 'EVAL' data split defined during training.

### 5. Collect BQML Metrics

*   **Component Function:** `src.pipeline_2025.create_bqml_comp.collect_eval_metrics_bqml`
*   **Description:** Parses the evaluation metrics artifact produced by the "Evaluate BQML Model" step, logs key metrics to the KFP UI, and returns them as individual outputs.
*   **Inputs:**
    *   `eval_metrics_artifact` (Artifact): The evaluation metrics artifact from the "Evaluate BQML Model" step (`bqml_evaluate_task.outputs["evaluation_metrics"]`).
*   **Outputs (as `NamedTuple` and KFP scalar metrics):**
    *   `mean_absolute_error` (float)
    *   `mean_squared_error` (float)
    *   `root_mean_squared_error` (float)
    *   `r2_score` (float)
    *   `median_absolute_error` (float)
    *   `framework` (str, value: "BQML")
*   **Key Operations:**
    *   Reads the `metadata` from the input artifact.
    *   Parses specific metric values (MAE, MSE, RÂ² Score, Median Absolute Error).
    *   Calculates RMSE from MSE.
    *   Logs these metrics using `metrics.log_metric()` for display in the Vertex AI Pipelines UI.

## AutoML Branch Components

### 6. Create Vertex AI Dataset

*   **Component Function:** `google_cloud_pipeline_components.v1.dataset.TabularDatasetCreateOp` (Pre-built GCPC component)
*   **Description:** Creates a Vertex AI tabular dataset from the preprocessed BigQuery table.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `display_name` (str): Display name for the dataset.
    *   `bq_source` (str): BigQuery table source as a URI (e.g., `bq://project.dataset.table`).
    *   `location` (str): GCP region.
*   **Outputs:**
    *   `dataset` (Artifact): The created Vertex AI Dataset artifact.
*   **Key Operations:**
    *   Creates a Vertex AI TabularDataset resource from the BigQuery table.

### 7. Train AutoML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.automl.training_job.AutoMLTabularTrainingJobRunOp` (Pre-built GCPC component)
*   **Description:** Trains an AutoML tabular model using the Vertex AI dataset.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `display_name` (str): Display name for the training job.
    *   `optimization_prediction_type` (str): "regression" for this case.
    *   `optimization_objective` (str): "minimize-rmse" for this regression task.
    *   `budget_milli_node_hours` (int): Training budget in milli node hours.
    *   `model_display_name` (str): Display name for the resulting model.
    *   `dataset` (Artifact): The Vertex AI dataset from the previous step.
    *   `target_column` (str): Name of the target column (e.g., "weight_pounds").
    *   `column_specs` (dict): Specifications for how to treat each column.
    *   `location` (str): GCP region.
*   **Outputs:**
    *   `model` (Artifact): The trained AutoML model artifact.
*   **Key Operations:**
    *   Trains an AutoML tabular regression model on the dataset.
    *   Registers the model in Vertex AI Model Registry.

### 8. Collect AutoML Metrics

*   **Component Function:** `src.pipeline_2025.create_automl_comp.collect_eval_metrics_automl`
*   **Description:** Extracts evaluation metrics from the trained AutoML model.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `region` (str): GCP region.
    *   `model_artifact` (Artifact): The trained model artifact from the "Train AutoML Model" step.
*   **Outputs (as `NamedTuple` and KFP scalar metrics):**
    *   `mean_absolute_error` (float)
    *   `mean_squared_error` (float)
    *   `root_mean_squared_error` (float)
    *   `r2_score` (float)
    *   `median_absolute_error` (float) (set to 0.0 if not available from AutoML)
    *   `framework` (str, value: "AutoML")
*   **Key Operations:**
    *   Uses the Vertex AI SDK to load the model and fetch its evaluations.
    *   Maps AutoML metric names to consistent output names.
    *   Extracts the available metrics from the evaluation.
    *   Calculates MSE from RMSE (as AutoML provides RMSE directly).
    *   Implements timeout handling to prevent component hanging.
    *   Includes robust error handling to ensure component always completes.
    *   Logs all metrics for display in the Vertex AI Pipelines UI.

## Model Selection Component

### 9. Select Best Model

*   **Component Function:** `src.pipeline_2025.select_best_model_comp.select_best_model`
*   **Description:** Compares the metrics from both BQML and AutoML models to select the best performing model based on a specified metric.
*   **Inputs:**
    *   `automl_metrics` (Metrics): Metrics from the AutoML model evaluation.
    *   `automl_model` (Artifact): The trained AutoML model artifact.
    *   `bqml_metrics` (Metrics): Metrics from the BQML model evaluation.
    *   `bqml_model` (Artifact): The trained BQML model artifact.
    *   `reference_metric_name` (str): The metric to use for comparison (e.g., "mean_absolute_error", "root_mean_squared_error", "r2_score").
    *   `thresholds_dict` (dict): Dictionary of threshold values for deployment decision.
*   **Outputs (as `NamedTuple`):**
    *   `deploy_decision` (str): "true" if the best model meets the threshold criteria, "false" otherwise.
    *   `best_model_name` (str): Name of the better performing model ("BQML" or "AutoML").
    *   `best_metric_value` (float): The value of the reference metric for the best model.
*   **Key Operations:**
    *   Intelligently compares metrics based on whether lower values are better (MAE, MSE, RMSE) or higher values are better (RÂ²).
    *   Handles missing metrics with safe defaults.
    *   Determines whether the best model meets deployment thresholds.
    *   Provides detailed logging of the comparison and decision-making process.

## Endpoint Management Components

### 10. Get or Create Endpoint

*   **Component Function:** `src.pipeline_2025.endpoint_management_comp.get_or_create_endpoint`
*   **Description:** Checks for an existing endpoint with the given display name and creates one if none exists. This prevents creating duplicate endpoints in production.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `location` (str): GCP region for the endpoint.
    *   `display_name` (str): Display name for the endpoint.
*   **Outputs:**
    *   `endpoint` (Artifact): The Vertex AI Endpoint artifact.
    *   `endpoint_resource_name` (str): The full resource name of the endpoint.
    *   `is_new_endpoint` (bool): Whether a new endpoint was created (false if using existing endpoint).
*   **Key Operations:**
    *   Checks for existing endpoints with the specified display name.
    *   Uses the most recently created endpoint if multiple exist.
    *   Creates a new endpoint only if no matching endpoint exists.
    *   Returns information about whether the endpoint is new or existing.

### 11. Standard Endpoint Creation (for compatibility)

*   **Component Function:** `google_cloud_pipeline_components.v1.endpoint.EndpointCreateOp` (Pre-built GCPC component)
*   **Description:** Creates a Vertex AI Endpoint for model deployment. Used in parallel with the `get_or_create_endpoint` component to maintain backward compatibility.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): GCP region for the endpoint.
    *   `display_name` (str): Display name for the endpoint.
*   **Outputs:**
    *   `endpoint` (Artifact): The created Vertex AI Endpoint artifact.
*   **Key Operations:**
    *   Creates a Vertex AI Endpoint resource where models can be deployed.

## Model Registry Component

### 12. Register Best Model

*   **Component Function:** `src.pipeline_2025.model_registry_comp.register_best_model_in_registry`
*   **Description:** Registers the selected model (BQML or AutoML) in the Vertex AI Model Registry with proper metadata for lineage tracking.
*   **Inputs:**
    *   `model` (Artifact): The trained model artifact (either BQML or AutoML model).
    *   `model_name` (str): Name for the registered model.
    *   `model_version` (str): Version identifier for the model (typically timestamp-based).
    *   `metrics` (Metrics): The evaluation metrics for the model.
    *   `project_id` (str): GCP Project ID.
    *   `location` (str): GCP region.
    *   `description` (str): Description of the model.
    *   `framework` (str, optional): ML framework used (e.g., "tensorflow").
    *   `additional_metadata` (dict, optional): Additional metadata to associate with the model.
*   **Outputs:**
    *   `registered_model_id` (str): ID of the registered model.
    *   `model_version_id` (str): ID of the specific model version.
*   **Key Operations:**
    *   Registers the model in the Vertex AI Model Registry.
    *   Adds version information and metadata for tracking.
    *   Associates evaluation metrics with the registered model.
    *   Supports detailed metadata for ML governance and lineage tracking.

## Deployment Components

### 13. Deploy Model

*   **Component Function:** `google_cloud_pipeline_components.v1.model.ModelDeployOp` (Pre-built GCPC component)
*   **Description:** Deploys the selected model to the Vertex AI Endpoint. This component is conditionally executed based on the model selection results.
*   **Inputs:**
    *   `model` (Artifact): The trained model artifact (either BQML or AutoML model, depending on selection).
    *   `endpoint` (Artifact): The Vertex AI Endpoint artifact.
    *   `dedicated_resources_machine_type` (str): Machine type for the deployment (e.g., "n1-standard-2").
    *   `dedicated_resources_min_replica_count` (int): Minimum number of replicas for the deployment.
    *   `dedicated_resources_max_replica_count` (int): Maximum number of replicas for the deployment.
    *   `traffic_split` (dict): Traffic split configuration ({"0": 100} means 100% of traffic goes to this model).
*   **Outputs:**
    *   `deployed_model` (Artifact): The deployed model artifact.
*   **Key Operations:**
    *   Deploys the selected model to the Vertex AI Endpoint.
    *   Configures compute resources for the deployment.
    *   Only executed if the model meets the quality threshold defined in the model selection component.

### 14. Update Traffic Split

*   **Component Function:** `src.pipeline_2025.endpoint_management_comp.update_traffic_split`
*   **Description:** Updates the traffic split for an existing endpoint to route traffic to the newly deployed model.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `location` (str): GCP region.
    *   `endpoint_resource_name` (str): The full resource name of the endpoint.
    *   `deployed_model_id` (str): ID of the deployed model to route traffic to.
    *   `traffic_percentage` (int): Percentage of traffic to route to the model (default: 100%).
*   **Outputs:**
    *   `success` (bool): Whether the traffic update was successful.
*   **Key Operations:**
    *   Retrieves the current endpoint configuration.
    *   Identifies the most recently deployed model if a placeholder ID is provided.
    *   Gradually routes traffic to the new model.
    *   Distributes remaining traffic (if any) evenly among other deployed models.

## Conditional Execution

The pipeline uses conditional execution for deployment with modern KFP v2 control flow constructs:

1. **Model Quality Check** - Uses `dsl.If` to only deploy a model if `deploy_decision` is "true", meaning the model meets the quality threshold for the chosen metric.

2. **Model Type Branching** - Uses `dsl.If` and `dsl.Elif` for separate branches of AutoML and BQML model deployment, depending on which model performed better.

3. **Endpoint Management** - Uses `dsl.If` to conditionally update traffic for existing endpoints versus new endpoints.

This implementation follows best practices by using the more Pythonic control flow constructs introduced in KFP v2 (`dsl.If`/`dsl.Elif`/`dsl.Else`), which replace the deprecated `dsl.Condition` from KFP v1.

## Improvements in the ML Pipeline Architecture

Several enhancements were made to improve reliability, performance, and production-readiness:

1. **Endpoint Reuse:** The pipeline now checks for existing endpoints with the same name before creating new ones, preventing endpoint proliferation in production environments.

2. **Model Registry Integration:** All models are now properly registered in the Vertex AI Model Registry with metadata and lineage information, improving governance and traceability.

3. **Traffic Management:** The pipeline includes the ability to gradually shift traffic to new model versions, enabling blue/green deployments and minimizing service disruption.

4. **Metrics Standardization:** Both BQML and AutoML components return exactly the same metric structure, making downstream comparison easier.

5. **Robust API Interaction:** Components use the high-level Vertex AI SDK with proper exception handling for more reliable API interactions.

6. **Detailed Logging:** Comprehensive logging helps with debugging and tracking the pipeline execution.

7. **Conditional Deployment:** The pipeline includes conditional logic to only deploy models that meet quality thresholds, and to deploy the best-performing model.

8. **Caching Strategy:** The pipeline uses an intelligent caching strategy to optimize resource usage during development while ensuring unique versioning in production.

These improvements ensure that the pipeline runs efficiently and reliably, with consistent metric outputs from both model types for comparison, intelligent model selection, and production-ready deployment capabilities.

# BQML Model Training Component

The BQML model training component creates and trains a BigQuery ML model for baby weight prediction:

## Component Functionality

* Creates a DNN_LINEAR_COMBINED_REGRESSOR model in BigQuery ML
* Uses hyperparameter tuning to optimize model performance
* Registers the model directly with Vertex AI for deployment
* Uses a cache-friendly model ID approach to optimize pipeline execution

## Inputs

* `project_id` (str): GCP project ID
* `bq_dataset` (str): BigQuery dataset name for storing the model
* `bq_model_name` (str): Name for the BigQuery ML model
* `formatted_bq_version_aliases` (str): Formatted string for model version aliases
* `var_target` (str): Target column for prediction (weight_pounds)
* `bq_train_table_id` (str): Full path to preprocessed BigQuery training data table
* `model_registry` (str): Set to "vertex_ai" to register model directly with Vertex AI
* `vertex_ai_model_id` (str): ID for the model in Vertex AI Model Registry

## Implementation Details

The component uses the BigqueryCreateModelJobOp operator, providing a SQL query that:

1. Creates a DNN_LINEAR_COMBINED_REGRESSOR model  
2. Uses CUSTOM split method with 'custom_splits' column
3. Configures hyperparameter tuning with multiple trials
4. Directly registers the model with Vertex AI Model Registry
5. Uses a stable model ID when caching is enabled, or a unique timestamp-based ID when caching is disabled

The model is created with the following architecture:
* Hidden layer sizes: [256, 128, 64]
* Optimizer: Adagrad
* Hyperparameter tuning for batch size and dropout rate
* 24 trials with 4 parallel trials maximum

### Cache-Friendly Approach

To optimize pipeline execution and caching:
* When caching is enabled, we use a stable identifier "model-name-cached"
* When caching is disabled or in production, we use a unique identifier with timestamp

This approach allows:
* Fast development iterations using cached model training
* Unique model versions for production deployment
* Direct model deployment without export/upload steps

# Model Deployment

The pipeline uses a production-ready approach to model deployment:

1. BQML models are registered directly with Vertex AI during creation using the `model_registry='vertex_ai'` option
2. AutoML models are already registered with Vertex AI during training
3. The pipeline now checks for existing endpoints to avoid creating unnecessary duplicates
4. After model selection, the chosen model is registered with comprehensive metadata
5. The model is then deployed to the endpoint with appropriate compute resources
6. For existing endpoints, traffic is gradually shifted to the new model version
7. This approach enables blue/green deployments and minimizes service disruption during model updates 