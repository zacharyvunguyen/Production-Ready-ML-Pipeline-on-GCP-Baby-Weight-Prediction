# Baby MLOps Pipeline Overview

This document describes the components of the Kubeflow Pipeline (KFP) used for the Baby Weight Prediction project. The pipeline automates the process of data extraction, preprocessing, model training (using BigQuery ML), model evaluation, and metrics collection.

## Pipeline Flow

The pipeline consists of the following components executed sequentially:

1.  **Extract Source Data** (`extract_source_data`)
2.  **Preprocess and Split Data** (`preprocess_data_and_split`)
3.  **Train BQML Model** (`BigqueryCreateModelJobOp`)
4.  **Evaluate BQML Model** (`BigqueryEvaluateModelJobOp`)
5.  **Collect BQML Metrics** (`collect_eval_metrics_bqml`)

## Component Details

### 1. Extract Source Data

*   **Component Function:** `src.pipeline_2025.data_prep_comp.extract_source_data`
*   **Description:** Extracts data from the source BigQuery table (`bigquery-public-data.samples.natality`), filters it based on a specified year and other conditions (e.g., valid `weight_pounds`, `mother_age`), and stores the result in a new BigQuery table.
*   **Inputs (from pipeline parameters):**
    *   `project_id` (str): GCP Project ID.
    *   `source_bq_table_id` (str): Full ID of the source natality table.
    *   `extracted_bq_table_id` (str): Full ID for the output table where extracted data will be stored.
    *   `filter_year` (int): Year to filter data (e.g., extract records where `year > filter_year`).
    *   `region` (str): GCP region (explicitly uses "US" for the BQ job to access public data).
*   **Outputs:**
    *   `extracted_table_uri` (str): URI of the created BigQuery table (e.g., `bq://project.dataset.table`).
    *   `extracted_table_id` (str): Full ID of the created BigQuery table.
*   **Key Operations:**
    *   Runs a `CREATE OR REPLACE TABLE AS SELECT ...` BigQuery query.
    *   Selects relevant features and applies initial filters.
    *   Ensures the target dataset exists in the "US" location.

### 2. Preprocess and Split Data

*   **Component Function:** `src.pipeline_2025.data_prep_comp.preprocess_data_and_split`
*   **Description:** Takes the extracted data, performs further preprocessing and feature engineering, limits the dataset size, and splits the data into `TRAIN`, `VALIDATE`, and `TEST` sets using a hash-based method for reproducibility.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `input_bq_table_id` (str): Full ID of the table from the "Extract Source Data" step (passed from `extract_task.outputs["extracted_table_id"]`).
    *   `preprocessed_bq_table_id` (str): Full ID for the output table where preprocessed data will be stored.
    *   `data_limit` (int): Maximum number of rows to process.
    *   `region` (str): GCP region for the BigQuery job.
*   **Outputs:**
    *   `preprocessed_table_uri` (str): URI of the created preprocessed BigQuery table.
    *   `preprocessed_table_id` (str): Full ID of the created preprocessed BigQuery table.
*   **Key Operations:**
    *   Runs a `CREATE OR REPLACE TABLE AS SELECT ...` BigQuery query.
    *   Casts boolean columns to `STRING`.
    *   Transforms `plurality` into categories (e.g., "Single(1)", "Twins(2)").
    *   Handles `NULL` values for `cigarette_use` and `alcohol_use` by casting to `STRING` and replacing `NULL` with "Unknown".
    *   Creates a `hash_values` column based on several features for reproducible data splitting.
    *   Creates a `data_split` column (`TRAIN`, `VALIDATE`, `TEST`) based on `MOD(hash_values, 10)`.

### 3. Train BQML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.bigquery.BigqueryCreateModelJobOp` (Pre-built GCPC component)
*   **Description:** Trains a BigQuery ML model using the preprocessed data. The specific model type and training options are defined in a SQL query string.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): BigQuery location for the job (e.g., "US").
    *   `query` (str): A SQL `CREATE MODEL` statement. This query is dynamically generated by `src.pipeline_2025.create_bqml_comp.create_query_build_bqml_model` using:
        *   `project_id`
        *   `bq_dataset` (staging dataset name from config)
        *   `bq_model_name` (from pipeline parameters)
        *   `formatted_bq_version_aliases` (from pipeline parameters, e.g., "['v1','latest']")
        *   `var_target` (target variable name from pipeline parameters, e.g., "weight_pounds")
        *   `bq_train_table_id` (the preprocessed table from `preprocess_task.outputs["preprocessed_table_id"]`)
*   **Outputs (Standard for `BigqueryCreateModelJobOp`):**
    *   `model` (Artifact): An artifact representing the trained BQML model, registered in Vertex AI Model Registry. Its URI points to the Vertex AI Model resource.
    *   Other outputs like `job_url`.
*   **Key Operations:**
    *   Executes the `CREATE MODEL` query in BigQuery.
    *   The model specified is a `DNN_LINEAR_COMBINED_REGRESSOR`.
    *   Uses `model_registry = 'vertex_ai'` to register the model with Vertex AI.
    *   Assigns `vertex_ai_model_version_aliases`.
    *   Uses `data_split_method = 'CUSTOM'` with `data_split_col = 'custom_splits'` (remapped from the `data_split` column where 'VALIDATE' becomes 'EVAL' for BQML).
    *   Includes hyperparameter tuning options (`HPARAM_CANDIDATES`, `MAX_ITERATIONS`, `NUM_TRIALS`, etc.).

### 4. Evaluate BQML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.bigquery.BigqueryEvaluateModelJobOp` (Pre-built GCPC component)
*   **Description:** Evaluates the trained BQML model.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): BigQuery location (same as training).
    *   `model` (Artifact): The trained model artifact from the "Train BQML Model" step (`bqml_train_task.outputs["model"]`).
*   **Outputs (Standard for `BigqueryEvaluateModelJobOp`):**
    *   `evaluation_metrics` (Artifact): An artifact containing the evaluation metrics from `ML.EVALUATE`.
    *   Other outputs like `job_url`.
*   **Key Operations:**
    *   Runs an `ML.EVALUATE` query on the trained BQML model using the 'EVAL' data split defined during training.

### 5. Collect BQML Metrics

*   **Component Function:** `src.pipeline_2025.create_bqml_comp.collect_eval_metrics_bqml`
*   **Description:** Parses the evaluation metrics artifact produced by the "Evaluate BQML Model" step, logs key metrics to the KFP UI, and returns them as individual outputs.
*   **Inputs:**
    *   `eval_metrics_artifact` (Artifact): The evaluation metrics artifact from the "Evaluate BQML Model" step (`bqml_evaluate_task.outputs["evaluation_metrics"]`).
*   **Outputs (as `NamedTuple` and KFP scalar metrics):**
    *   `mean_absolute_error` (float)
    *   `mean_squared_error` (float)
    *   `root_mean_squared_error` (float)
    *   `r2_score` (float)
    *   `median_absolute_error` (float)
    *   `framework` (str, value: "BQML")
*   **Key Operations:**
    *   Reads the `metadata` from the input artifact.
    *   Parses specific metric values (MAE, MSE, RÂ² Score, Median Absolute Error).
    *   Calculates RMSE from MSE.
    *   Logs these metrics using `metrics.log_metric()` for display in the Vertex AI Pipelines UI. 