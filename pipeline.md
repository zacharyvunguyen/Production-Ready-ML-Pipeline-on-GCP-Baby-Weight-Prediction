# Baby MLOps Pipeline Overview

This document describes the components of the Kubeflow Pipeline (KFP) used for the Baby Weight Prediction project. The pipeline includes two parallel branches: a BigQuery ML (BQML) branch and an AutoML branch, both performing data extraction, preprocessing, model training, evaluation, and metrics collection.

## Pipeline Flow

The pipeline consists of the following components:

1. **Extract Source Data** (`extract_source_data`)
2. **Preprocess and Split Data** (`preprocess_data_and_split`)

**BQML Branch:**
3. **Train BQML Model** (`BigqueryCreateModelJobOp`)
4. **Evaluate BQML Model** (`BigqueryEvaluateModelJobOp`)
5. **Collect BQML Metrics** (`collect_eval_metrics_bqml`)

**AutoML Branch:**
6. **Create Vertex AI Dataset** (`TabularDatasetCreateOp`)
7. **Train AutoML Model** (`AutoMLTabularTrainingJobRunOp`)
8. **Collect AutoML Metrics** (`collect_eval_metrics_automl`)

## Component Details

### 1. Extract Source Data

*   **Component Function:** `src.pipeline_2025.data_prep_comp.extract_source_data`
*   **Description:** Extracts data from the source BigQuery table (`bigquery-public-data.samples.natality`), filters it based on a specified year and other conditions (e.g., valid `weight_pounds`, `mother_age`), and stores the result in a new BigQuery table.
*   **Inputs (from pipeline parameters):**
    *   `project_id` (str): GCP Project ID.
    *   `source_bq_table_id` (str): Full ID of the source natality table.
    *   `extracted_bq_table_id` (str): Full ID for the output table where extracted data will be stored.
    *   `filter_year` (int): Year to filter data (e.g., extract records where `year > filter_year`).
    *   `region` (str): GCP region (explicitly uses "US" for the BQ job to access public data).
*   **Outputs:**
    *   `extracted_table_uri` (str): URI of the created BigQuery table (e.g., `bq://project.dataset.table`).
    *   `extracted_table_id` (str): Full ID of the created BigQuery table.
*   **Key Operations:**
    *   Runs a `CREATE OR REPLACE TABLE AS SELECT ...` BigQuery query.
    *   Selects relevant features and applies initial filters.
    *   Ensures the target dataset exists in the "US" location.

### 2. Preprocess and Split Data

*   **Component Function:** `src.pipeline_2025.data_prep_comp.preprocess_data_and_split`
*   **Description:** Takes the extracted data, performs further preprocessing and feature engineering, limits the dataset size, and splits the data into `TRAIN`, `VALIDATE`, and `TEST` sets using a hash-based method for reproducibility.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `input_bq_table_id` (str): Full ID of the table from the "Extract Source Data" step (passed from `extract_task.outputs["extracted_table_id"]`).
    *   `preprocessed_bq_table_id` (str): Full ID for the output table where preprocessed data will be stored.
    *   `data_limit` (int): Maximum number of rows to process.
    *   `region` (str): GCP region for the BigQuery job.
*   **Outputs:**
    *   `preprocessed_table_uri` (str): URI of the created preprocessed BigQuery table.
    *   `preprocessed_table_id` (str): Full ID of the created preprocessed BigQuery table.
*   **Key Operations:**
    *   Runs a `CREATE OR REPLACE TABLE AS SELECT ...` BigQuery query.
    *   Casts boolean columns to `STRING`.
    *   Transforms `plurality` into categories (e.g., "Single(1)", "Twins(2)").
    *   Handles `NULL` values for `cigarette_use` and `alcohol_use` by casting to `STRING` and replacing `NULL` with "Unknown".
    *   Creates a `hash_values` column based on several features for reproducible data splitting.
    *   Creates a `data_split` column (`TRAIN`, `VALIDATE`, `TEST`) based on `MOD(hash_values, 10)`.

## BQML Branch Components

### 3. Train BQML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.bigquery.BigqueryCreateModelJobOp` (Pre-built GCPC component)
*   **Description:** Trains a BigQuery ML model using the preprocessed data. The specific model type and training options are defined in a SQL query string.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): BigQuery location for the job (e.g., "US").
    *   `query` (str): A SQL `CREATE MODEL` statement. This query is dynamically generated by `src.pipeline_2025.create_bqml_comp.create_query_build_bqml_model` using:
        *   `project_id`
        *   `bq_dataset` (staging dataset name from config)
        *   `bq_model_name` (from pipeline parameters)
        *   `formatted_bq_version_aliases` (from pipeline parameters, e.g., "['v1','latest']")
        *   `var_target` (target variable name from pipeline parameters, e.g., "weight_pounds")
        *   `bq_train_table_id` (the preprocessed table from `preprocess_task.outputs["preprocessed_table_id"]`)
*   **Outputs (Standard for `BigqueryCreateModelJobOp`):**
    *   `model` (Artifact): An artifact representing the trained BQML model, registered in Vertex AI Model Registry. Its URI points to the Vertex AI Model resource.
    *   Other outputs like `job_url`.
*   **Key Operations:**
    *   Executes the `CREATE MODEL` query in BigQuery.
    *   The model specified is a `DNN_LINEAR_COMBINED_REGRESSOR`.
    *   Uses `model_registry = 'vertex_ai'` to register the model with Vertex AI.
    *   Assigns `vertex_ai_model_version_aliases`.
    *   Uses `data_split_method = 'CUSTOM'` with `data_split_col = 'custom_splits'` (remapped from the `data_split` column where 'VALIDATE' becomes 'EVAL' for BQML).
    *   Includes hyperparameter tuning options (`HPARAM_CANDIDATES`, `MAX_ITERATIONS`, `NUM_TRIALS`, etc.).

### 4. Evaluate BQML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.bigquery.BigqueryEvaluateModelJobOp` (Pre-built GCPC component)
*   **Description:** Evaluates the trained BQML model.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `location` (str): BigQuery location (same as training).
    *   `model` (Artifact): The trained model artifact from the "Train BQML Model" step (`bqml_train_task.outputs["model"]`).
*   **Outputs (Standard for `BigqueryEvaluateModelJobOp`):**
    *   `evaluation_metrics` (Artifact): An artifact containing the evaluation metrics from `ML.EVALUATE`.
    *   Other outputs like `job_url`.
*   **Key Operations:**
    *   Runs an `ML.EVALUATE` query on the trained BQML model using the 'EVAL' data split defined during training.

### 5. Collect BQML Metrics

*   **Component Function:** `src.pipeline_2025.create_bqml_comp.collect_eval_metrics_bqml`
*   **Description:** Parses the evaluation metrics artifact produced by the "Evaluate BQML Model" step, logs key metrics to the KFP UI, and returns them as individual outputs.
*   **Inputs:**
    *   `eval_metrics_artifact` (Artifact): The evaluation metrics artifact from the "Evaluate BQML Model" step (`bqml_evaluate_task.outputs["evaluation_metrics"]`).
*   **Outputs (as `NamedTuple` and KFP scalar metrics):**
    *   `mean_absolute_error` (float)
    *   `mean_squared_error` (float)
    *   `root_mean_squared_error` (float)
    *   `r2_score` (float)
    *   `median_absolute_error` (float)
    *   `framework` (str, value: "BQML")
*   **Key Operations:**
    *   Reads the `metadata` from the input artifact.
    *   Parses specific metric values (MAE, MSE, RÂ² Score, Median Absolute Error).
    *   Calculates RMSE from MSE.
    *   Logs these metrics using `metrics.log_metric()` for display in the Vertex AI Pipelines UI.

## AutoML Branch Components

### 6. Create Vertex AI Dataset

*   **Component Function:** `google_cloud_pipeline_components.v1.dataset.TabularDatasetCreateOp` (Pre-built GCPC component)
*   **Description:** Creates a Vertex AI tabular dataset from the preprocessed BigQuery table.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `display_name` (str): Display name for the dataset.
    *   `bq_source` (str): BigQuery table source as a URI (e.g., `bq://project.dataset.table`).
    *   `location` (str): GCP region.
*   **Outputs:**
    *   `dataset` (Artifact): The created Vertex AI Dataset artifact.
*   **Key Operations:**
    *   Creates a Vertex AI TabularDataset resource from the BigQuery table.

### 7. Train AutoML Model

*   **Component Function:** `google_cloud_pipeline_components.v1.automl.training_job.AutoMLTabularTrainingJobRunOp` (Pre-built GCPC component)
*   **Description:** Trains an AutoML tabular model using the Vertex AI dataset.
*   **Inputs:**
    *   `project` (str): GCP Project ID.
    *   `display_name` (str): Display name for the training job.
    *   `optimization_prediction_type` (str): "regression" for this case.
    *   `optimization_objective` (str): "minimize-rmse" for this regression task.
    *   `budget_milli_node_hours` (int): Training budget in milli node hours.
    *   `model_display_name` (str): Display name for the resulting model.
    *   `dataset` (Artifact): The Vertex AI dataset from the previous step.
    *   `target_column` (str): Name of the target column (e.g., "weight_pounds").
    *   `column_specs` (dict): Specifications for how to treat each column.
    *   `location` (str): GCP region.
*   **Outputs:**
    *   `model` (Artifact): The trained AutoML model artifact.
*   **Key Operations:**
    *   Trains an AutoML tabular regression model on the dataset.
    *   Registers the model in Vertex AI Model Registry.

### 8. Collect AutoML Metrics

*   **Component Function:** `src.pipeline_2025.create_automl_comp.collect_eval_metrics_automl`
*   **Description:** Extracts evaluation metrics from the trained AutoML model.
*   **Inputs:**
    *   `project_id` (str): GCP Project ID.
    *   `region` (str): GCP region.
    *   `model_artifact` (Artifact): The trained model artifact from the "Train AutoML Model" step.
*   **Outputs (as `NamedTuple` and KFP scalar metrics):**
    *   `mean_absolute_error` (float)
    *   `mean_squared_error` (float)
    *   `root_mean_squared_error` (float)
    *   `r2_score` (float)
    *   `median_absolute_error` (float) (set to 0.0 if not available from AutoML)
    *   `framework` (str, value: "AutoML")
*   **Key Operations:**
    *   Uses the Vertex AI SDK to load the model and fetch its evaluations.
    *   Maps AutoML metric names to consistent output names.
    *   Extracts the available metrics from the evaluation.
    *   Calculates MSE from RMSE (as AutoML provides RMSE directly).
    *   Implements timeout handling to prevent component hanging.
    *   Includes robust error handling to ensure component always completes.
    *   Logs all metrics for display in the Vertex AI Pipelines UI.

## Improvements in the Metrics Collection Components

Several enhancements were made to improve reliability and performance:

1. **Timeout Mechanism:** A 3-minute timeout was implemented in the AutoML metrics collection component to prevent it from hanging if the API calls take too long.

2. **Error Handling:** Comprehensive error handling ensures the components continue execution even if there are issues accessing the metrics.

3. **Default Values:** All metrics are initialized with sensible default values (0.0) to ensure consistent outputs even when metrics are missing.

4. **Consistent Metrics Structure:** Both BQML and AutoML components return exactly the same metric structure, making downstream comparison easier.

5. **Robust API Interaction:** The AutoML component uses the high-level Vertex AI SDK with proper exception handling for more reliable API interactions.

6. **Logging Enhancement:** Detailed logging helps with debugging and tracking the metrics extraction process.

These improvements ensure that the pipeline runs efficiently and reliably, with consistent metric outputs from both model types for comparison. 